{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nichakornchaisuwan/Project_Boneage/blob/main/BoneAgePredictor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcZjvd_65QVN",
        "outputId": "3e1c2b7f-06a1-4f7b-ea50-dc2da8a121af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-1.9.0-py3-none-any.whl (825 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.8/825.8 KB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (23.0)\n",
            "Collecting torchmetrics>=0.7.0\n",
            "  Downloading torchmetrics-0.11.1-py3-none-any.whl (517 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.2/517.2 KB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (2023.1.0)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (1.21.6)\n",
            "Collecting lightning-utilities>=0.4.2\n",
            "  Downloading lightning_utilities-0.6.0.post0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (1.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (4.4.0)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (4.64.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (3.8.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (2.25.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.8.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (2.1.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (22.2.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2.10)\n",
            "Installing collected packages: lightning-utilities, torchmetrics, pytorch-lightning\n",
            "Successfully installed lightning-utilities-0.6.0.post0 pytorch-lightning-1.9.0 torchmetrics-0.11.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-lightning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "pduFNGhc55Dw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive # เชื่อม drive ของเรา ถ้าเชื่อมสำเร็จจะขึ้นคำว่าMounted at /content/drive \n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnsm5FPI6met",
        "outputId": "00e1f083-9d6d-41ba-f884-0dfe19ebeed1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_path(*rel_path):\n",
        "    return os.path.join('/content/drive/My Drive/Project_Boneage', *rel_path);"
      ],
      "metadata": {
        "id": "8SUUfnwf6qqC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "xGcSme2e-cjG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BoneAgeTrainingDataset(Dataset):\n",
        "    def __init__(self,csv_path,img_folder):\n",
        "        self.csv = pd.read_csv(dataset_path(csv_path));\n",
        "        self.img_folder = img_folder;        \n",
        "\n",
        "    def __len__(self):\n",
        "        return self.csv.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return transform(Image.open(dataset_path(self.img_folder,str(self.csv['id'][idx])+'.png')).resize((256,256))).double(),\\\n",
        "               torch.from_numpy(np.array(self.csv['boneage'][idx])).double()"
      ],
      "metadata": {
        "id": "yBJUXtkR691X"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BoneAgeTestingDataset(Dataset):\n",
        "    def __init__(self,csv_path,img_folder):\n",
        "        self.csv = pd.read_csv(dataset_path(csv_path));\n",
        "        self.img_folder = img_folder;        \n",
        "\n",
        "    def __len__(self):\n",
        "        return self.csv.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return transform(Image.open(dataset_path(self.img_folder,str(self.csv['id'][idx])+'.png')).resize((256,256))).double(),\\\n",
        "               torch.from_numpy(np.array(self.csv['boneage'][idx])).double()"
      ],
      "metadata": {
        "id": "D-PjS-wI0WxX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BoneAgePredictor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BoneAgePredictor, self).__init__()\n",
        "        # Layer 1\n",
        "        self.conv1 = nn.Conv2d(1, 4, 3)\n",
        "        nn.init.kaiming_normal_(self.conv1.weight)\n",
        "        self.batch1 = nn.BatchNorm2d(4)\n",
        "        # Layer 2\n",
        "        self.conv2 = nn.Conv2d(4, 8, 3)\n",
        "        nn.init.kaiming_normal_(self.conv2.weight)\n",
        "        self.batch2 = nn.BatchNorm2d(8)\n",
        "        # Layer 3\n",
        "        self.conv3 = nn.Conv2d(8, 16, 3)\n",
        "        nn.init.kaiming_normal_(self.conv3.weight)\n",
        "        self.batch3 = nn.BatchNorm2d(16)\n",
        "        # Layer 4\n",
        "        self.conv4 = nn.Conv2d(16, 16, 3)\n",
        "        nn.init.kaiming_normal_(self.conv4.weight)\n",
        "        self.batch4 = nn.BatchNorm2d(16)\n",
        "        # Layer 5\n",
        "        self.conv5 = nn.Conv2d(16, 16, 3)\n",
        "        nn.init.kaiming_normal_(self.conv5.weight)\n",
        "        self.batch5 = nn.BatchNorm2d(16)\n",
        "        # Fully connected\n",
        "        self.fc1 = nn.Linear(576, 24)\n",
        "        self.fc2 = nn.Linear(24, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Layer 1\n",
        "        x = F.relu(self.batch1(self.conv1(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        # Layer 2\n",
        "        x = F.relu(self.batch2(self.conv2(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        # Layer 3\n",
        "        x = F.relu(self.batch3(self.conv3(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        # Layer 4\n",
        "        x = F.relu(self.batch4(self.conv4(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        # Layer 5\n",
        "        x = F.relu(self.batch5(self.conv5(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        # Pooling\n",
        "        x = x.view(-1, 576)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "WgTn4ZQ3AwYh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.l1_loss(output.view(-1), target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))"
      ],
      "metadata": {
        "id": "UXdVbnBtFghc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, device, loader, loader_name):\n",
        "    model.eval()\n",
        "    loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            loss += F.l1_loss(output.view(-1), target, reduction='sum').item()  # sum up batch loss            \n",
        "    loss /= len(loader.dataset)\n",
        "    print('\\n', loader_name, 'set: Average loss: {:.4f}\\n'.format(loss))\n",
        "    return loss;"
      ],
      "metadata": {
        "id": "4UbDJqkSGIOY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "kwargs = {'num_workers': 2, 'pin_memory': True} if use_cuda else {}\n",
        "trainig_data_loader = torch.utils.data.DataLoader(\n",
        "    BoneAgeTrainingDataset('boneage-training-dataset.csv', 'boneage-training-dataset'),\n",
        "    batch_size=64, shuffle=True, **kwargs)\n",
        "testing_data_loader = torch.utils.data.DataLoader(\n",
        "    BoneAgeTestingDataset('boneage-test-dataset.csv', 'boneage-test-dataset'),\n",
        "    batch_size=64, shuffle=True, **kwargs)"
      ],
      "metadata": {
        "id": "ns7Mm3vQG1MH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BoneAgePredictor().double().to(device)\n",
        "print(model)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "scheduler = ReduceLROnPlateau(optimizer, factor=0.1, patience=10, min_lr=1e-6, verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPQsyeOTH0ny",
        "outputId": "447ecad2-5d50-468f-cec9-56069da38ebf"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BoneAgePredictor(\n",
            "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (batch1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv2): Conv2d(4, 8, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (batch2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv3): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (batch3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (batch4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv5): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (batch5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc1): Linear(in_features=576, out_features=24, bias=True)\n",
            "  (fc2): Linear(in_features=24, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, 4):\n",
        "        train(model, device, trainig_data_loader, optimizer, epoch)\n",
        "        train_loss = test(model, device, trainig_data_loader,'Train')\n",
        "        test_loss = test(model, device, testing_data_loader,'Test')\n",
        "        scheduler.step(test_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sizddBfdIOdf",
        "outputId": "f335d79d-2d2f-46ca-e78b-e2758c9b96a4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/12611 (0%)]\tLoss: 133.002815\n",
            "Train Epoch: 1 [64/12611 (1%)]\tLoss: 140.609259\n",
            "Train Epoch: 1 [128/12611 (1%)]\tLoss: 122.567861\n",
            "Train Epoch: 1 [192/12611 (2%)]\tLoss: 124.861490\n",
            "Train Epoch: 1 [256/12611 (2%)]\tLoss: 119.612031\n",
            "Train Epoch: 1 [320/12611 (3%)]\tLoss: 123.951518\n",
            "Train Epoch: 1 [384/12611 (3%)]\tLoss: 120.850074\n",
            "Train Epoch: 1 [448/12611 (4%)]\tLoss: 130.199654\n",
            "Train Epoch: 1 [512/12611 (4%)]\tLoss: 121.556820\n",
            "Train Epoch: 1 [576/12611 (5%)]\tLoss: 118.641101\n",
            "Train Epoch: 1 [640/12611 (5%)]\tLoss: 119.615149\n",
            "Train Epoch: 1 [704/12611 (6%)]\tLoss: 118.838312\n",
            "Train Epoch: 1 [768/12611 (6%)]\tLoss: 111.837461\n",
            "Train Epoch: 1 [832/12611 (7%)]\tLoss: 106.451316\n",
            "Train Epoch: 1 [896/12611 (7%)]\tLoss: 105.496016\n",
            "Train Epoch: 1 [960/12611 (8%)]\tLoss: 105.645408\n",
            "Train Epoch: 1 [1024/12611 (8%)]\tLoss: 105.205217\n",
            "Train Epoch: 1 [1088/12611 (9%)]\tLoss: 98.619011\n",
            "Train Epoch: 1 [1152/12611 (9%)]\tLoss: 101.961671\n",
            "Train Epoch: 1 [1216/12611 (10%)]\tLoss: 105.484132\n",
            "Train Epoch: 1 [1280/12611 (10%)]\tLoss: 101.468007\n",
            "Train Epoch: 1 [1344/12611 (11%)]\tLoss: 93.382199\n",
            "Train Epoch: 1 [1408/12611 (11%)]\tLoss: 91.305309\n",
            "Train Epoch: 1 [1472/12611 (12%)]\tLoss: 84.022502\n",
            "Train Epoch: 1 [1536/12611 (12%)]\tLoss: 84.720500\n",
            "Train Epoch: 1 [1600/12611 (13%)]\tLoss: 83.568995\n",
            "Train Epoch: 1 [1664/12611 (13%)]\tLoss: 79.076073\n",
            "Train Epoch: 1 [1728/12611 (14%)]\tLoss: 79.549022\n",
            "Train Epoch: 1 [1792/12611 (14%)]\tLoss: 74.943603\n",
            "Train Epoch: 1 [1856/12611 (15%)]\tLoss: 75.265465\n",
            "Train Epoch: 1 [1920/12611 (15%)]\tLoss: 74.988227\n",
            "Train Epoch: 1 [1984/12611 (16%)]\tLoss: 64.692949\n",
            "Train Epoch: 1 [2048/12611 (16%)]\tLoss: 60.551313\n",
            "Train Epoch: 1 [2112/12611 (17%)]\tLoss: 59.080700\n",
            "Train Epoch: 1 [2176/12611 (17%)]\tLoss: 62.819382\n",
            "Train Epoch: 1 [2240/12611 (18%)]\tLoss: 50.140095\n",
            "Train Epoch: 1 [2304/12611 (18%)]\tLoss: 58.523775\n",
            "Train Epoch: 1 [2368/12611 (19%)]\tLoss: 51.134101\n",
            "Train Epoch: 1 [2432/12611 (19%)]\tLoss: 54.651561\n",
            "Train Epoch: 1 [2496/12611 (20%)]\tLoss: 47.259832\n",
            "Train Epoch: 1 [2560/12611 (20%)]\tLoss: 50.669983\n",
            "Train Epoch: 1 [2624/12611 (21%)]\tLoss: 43.467050\n",
            "Train Epoch: 1 [2688/12611 (21%)]\tLoss: 43.331897\n",
            "Train Epoch: 1 [2752/12611 (22%)]\tLoss: 46.017087\n",
            "Train Epoch: 1 [2816/12611 (22%)]\tLoss: 38.046398\n",
            "Train Epoch: 1 [2880/12611 (23%)]\tLoss: 47.443939\n",
            "Train Epoch: 1 [2944/12611 (23%)]\tLoss: 45.686928\n",
            "Train Epoch: 1 [3008/12611 (24%)]\tLoss: 34.002795\n",
            "Train Epoch: 1 [3072/12611 (24%)]\tLoss: 34.878666\n",
            "Train Epoch: 1 [3136/12611 (25%)]\tLoss: 34.285984\n",
            "Train Epoch: 1 [3200/12611 (25%)]\tLoss: 41.544631\n",
            "Train Epoch: 1 [3264/12611 (26%)]\tLoss: 38.861056\n",
            "Train Epoch: 1 [3328/12611 (26%)]\tLoss: 37.938962\n",
            "Train Epoch: 1 [3392/12611 (27%)]\tLoss: 34.298792\n",
            "Train Epoch: 1 [3456/12611 (27%)]\tLoss: 37.905952\n",
            "Train Epoch: 1 [3520/12611 (28%)]\tLoss: 34.319527\n",
            "Train Epoch: 1 [3584/12611 (28%)]\tLoss: 31.993811\n",
            "Train Epoch: 1 [3648/12611 (29%)]\tLoss: 29.106912\n",
            "Train Epoch: 1 [3712/12611 (29%)]\tLoss: 33.569971\n",
            "Train Epoch: 1 [3776/12611 (30%)]\tLoss: 29.705363\n",
            "Train Epoch: 1 [3840/12611 (30%)]\tLoss: 27.953642\n",
            "Train Epoch: 1 [3904/12611 (31%)]\tLoss: 35.500090\n",
            "Train Epoch: 1 [3968/12611 (31%)]\tLoss: 38.504556\n",
            "Train Epoch: 1 [4032/12611 (32%)]\tLoss: 34.692894\n",
            "Train Epoch: 1 [4096/12611 (32%)]\tLoss: 25.082227\n",
            "Train Epoch: 1 [4160/12611 (33%)]\tLoss: 36.962129\n",
            "Train Epoch: 1 [4224/12611 (33%)]\tLoss: 36.809930\n",
            "Train Epoch: 1 [4288/12611 (34%)]\tLoss: 30.619168\n",
            "Train Epoch: 1 [4352/12611 (34%)]\tLoss: 34.347953\n",
            "Train Epoch: 1 [4416/12611 (35%)]\tLoss: 37.725676\n",
            "Train Epoch: 1 [4480/12611 (35%)]\tLoss: 32.028930\n",
            "Train Epoch: 1 [4544/12611 (36%)]\tLoss: 32.975110\n",
            "Train Epoch: 1 [4608/12611 (36%)]\tLoss: 37.441502\n",
            "Train Epoch: 1 [4672/12611 (37%)]\tLoss: 35.636764\n",
            "Train Epoch: 1 [4736/12611 (37%)]\tLoss: 34.614226\n",
            "Train Epoch: 1 [4800/12611 (38%)]\tLoss: 30.850812\n",
            "Train Epoch: 1 [4864/12611 (38%)]\tLoss: 34.008135\n",
            "Train Epoch: 1 [4928/12611 (39%)]\tLoss: 31.144232\n",
            "Train Epoch: 1 [4992/12611 (39%)]\tLoss: 31.276175\n",
            "Train Epoch: 1 [5056/12611 (40%)]\tLoss: 40.126263\n",
            "Train Epoch: 1 [5120/12611 (40%)]\tLoss: 29.532832\n",
            "Train Epoch: 1 [5184/12611 (41%)]\tLoss: 29.632015\n",
            "Train Epoch: 1 [5248/12611 (41%)]\tLoss: 29.502038\n",
            "Train Epoch: 1 [5312/12611 (42%)]\tLoss: 29.217218\n",
            "Train Epoch: 1 [5376/12611 (42%)]\tLoss: 28.862644\n",
            "Train Epoch: 1 [5440/12611 (43%)]\tLoss: 34.293435\n",
            "Train Epoch: 1 [5504/12611 (43%)]\tLoss: 30.851170\n",
            "Train Epoch: 1 [5568/12611 (44%)]\tLoss: 31.934007\n",
            "Train Epoch: 1 [5632/12611 (44%)]\tLoss: 26.271657\n",
            "Train Epoch: 1 [5696/12611 (45%)]\tLoss: 34.938059\n",
            "Train Epoch: 1 [5760/12611 (45%)]\tLoss: 26.852992\n",
            "Train Epoch: 1 [5824/12611 (46%)]\tLoss: 28.887671\n",
            "Train Epoch: 1 [5888/12611 (46%)]\tLoss: 26.931728\n",
            "Train Epoch: 1 [5952/12611 (47%)]\tLoss: 35.698843\n",
            "Train Epoch: 1 [6016/12611 (47%)]\tLoss: 31.751064\n",
            "Train Epoch: 1 [6080/12611 (48%)]\tLoss: 25.029361\n",
            "Train Epoch: 1 [6144/12611 (48%)]\tLoss: 32.306647\n",
            "Train Epoch: 1 [6208/12611 (49%)]\tLoss: 24.992893\n",
            "Train Epoch: 1 [6272/12611 (49%)]\tLoss: 32.045022\n",
            "Train Epoch: 1 [6336/12611 (50%)]\tLoss: 29.830853\n",
            "Train Epoch: 1 [6400/12611 (51%)]\tLoss: 29.523468\n",
            "Train Epoch: 1 [6464/12611 (51%)]\tLoss: 30.944203\n",
            "Train Epoch: 1 [6528/12611 (52%)]\tLoss: 28.119241\n",
            "Train Epoch: 1 [6592/12611 (52%)]\tLoss: 34.032540\n",
            "Train Epoch: 1 [6656/12611 (53%)]\tLoss: 31.297896\n",
            "Train Epoch: 1 [6720/12611 (53%)]\tLoss: 35.759094\n",
            "Train Epoch: 1 [6784/12611 (54%)]\tLoss: 39.578061\n",
            "Train Epoch: 1 [6848/12611 (54%)]\tLoss: 25.480880\n",
            "Train Epoch: 1 [6912/12611 (55%)]\tLoss: 28.414196\n",
            "Train Epoch: 1 [6976/12611 (55%)]\tLoss: 30.545965\n",
            "Train Epoch: 1 [7040/12611 (56%)]\tLoss: 33.505127\n",
            "Train Epoch: 1 [7104/12611 (56%)]\tLoss: 30.570378\n",
            "Train Epoch: 1 [7168/12611 (57%)]\tLoss: 31.787350\n",
            "Train Epoch: 1 [7232/12611 (57%)]\tLoss: 29.025927\n",
            "Train Epoch: 1 [7296/12611 (58%)]\tLoss: 24.929135\n",
            "Train Epoch: 1 [7360/12611 (58%)]\tLoss: 26.803265\n",
            "Train Epoch: 1 [7424/12611 (59%)]\tLoss: 36.657894\n",
            "Train Epoch: 1 [7488/12611 (59%)]\tLoss: 36.609423\n",
            "Train Epoch: 1 [7552/12611 (60%)]\tLoss: 27.143481\n",
            "Train Epoch: 1 [7616/12611 (60%)]\tLoss: 30.480209\n",
            "Train Epoch: 1 [7680/12611 (61%)]\tLoss: 26.472977\n",
            "Train Epoch: 1 [7744/12611 (61%)]\tLoss: 29.986252\n",
            "Train Epoch: 1 [7808/12611 (62%)]\tLoss: 30.743141\n",
            "Train Epoch: 1 [7872/12611 (62%)]\tLoss: 30.392039\n",
            "Train Epoch: 1 [7936/12611 (63%)]\tLoss: 30.166237\n",
            "Train Epoch: 1 [8000/12611 (63%)]\tLoss: 31.677216\n",
            "Train Epoch: 1 [8064/12611 (64%)]\tLoss: 28.451329\n",
            "Train Epoch: 1 [8128/12611 (64%)]\tLoss: 28.032790\n",
            "Train Epoch: 1 [8192/12611 (65%)]\tLoss: 29.110967\n",
            "Train Epoch: 1 [8256/12611 (65%)]\tLoss: 33.799084\n",
            "Train Epoch: 1 [8320/12611 (66%)]\tLoss: 31.426191\n",
            "Train Epoch: 1 [8384/12611 (66%)]\tLoss: 27.711483\n",
            "Train Epoch: 1 [8448/12611 (67%)]\tLoss: 26.113601\n",
            "Train Epoch: 1 [8512/12611 (67%)]\tLoss: 32.052237\n",
            "Train Epoch: 1 [8576/12611 (68%)]\tLoss: 29.426536\n",
            "Train Epoch: 1 [8640/12611 (68%)]\tLoss: 34.227901\n",
            "Train Epoch: 1 [8704/12611 (69%)]\tLoss: 32.594768\n",
            "Train Epoch: 1 [8768/12611 (69%)]\tLoss: 28.472747\n",
            "Train Epoch: 1 [8832/12611 (70%)]\tLoss: 32.627160\n",
            "Train Epoch: 1 [8896/12611 (70%)]\tLoss: 28.180314\n",
            "Train Epoch: 1 [8960/12611 (71%)]\tLoss: 34.310503\n",
            "Train Epoch: 1 [9024/12611 (71%)]\tLoss: 27.569995\n",
            "Train Epoch: 1 [9088/12611 (72%)]\tLoss: 28.687770\n",
            "Train Epoch: 1 [9152/12611 (72%)]\tLoss: 28.904068\n",
            "Train Epoch: 1 [9216/12611 (73%)]\tLoss: 29.889843\n",
            "Train Epoch: 1 [9280/12611 (73%)]\tLoss: 28.890205\n",
            "Train Epoch: 1 [9344/12611 (74%)]\tLoss: 27.907904\n",
            "Train Epoch: 1 [9408/12611 (74%)]\tLoss: 35.552644\n",
            "Train Epoch: 1 [9472/12611 (75%)]\tLoss: 31.028607\n",
            "Train Epoch: 1 [9536/12611 (75%)]\tLoss: 29.268447\n",
            "Train Epoch: 1 [9600/12611 (76%)]\tLoss: 31.703615\n",
            "Train Epoch: 1 [9664/12611 (76%)]\tLoss: 31.754952\n",
            "Train Epoch: 1 [9728/12611 (77%)]\tLoss: 26.320586\n",
            "Train Epoch: 1 [9792/12611 (77%)]\tLoss: 27.302642\n",
            "Train Epoch: 1 [9856/12611 (78%)]\tLoss: 29.612377\n",
            "Train Epoch: 1 [9920/12611 (78%)]\tLoss: 30.094444\n",
            "Train Epoch: 1 [9984/12611 (79%)]\tLoss: 25.416301\n",
            "Train Epoch: 1 [10048/12611 (79%)]\tLoss: 27.336117\n",
            "Train Epoch: 1 [10112/12611 (80%)]\tLoss: 32.187132\n",
            "Train Epoch: 1 [10176/12611 (80%)]\tLoss: 28.904035\n",
            "Train Epoch: 1 [10240/12611 (81%)]\tLoss: 29.599955\n",
            "Train Epoch: 1 [10304/12611 (81%)]\tLoss: 31.754963\n",
            "Train Epoch: 1 [10368/12611 (82%)]\tLoss: 27.503017\n",
            "Train Epoch: 1 [10432/12611 (82%)]\tLoss: 31.089572\n",
            "Train Epoch: 1 [10496/12611 (83%)]\tLoss: 30.681338\n",
            "Train Epoch: 1 [10560/12611 (83%)]\tLoss: 31.697036\n",
            "Train Epoch: 1 [10624/12611 (84%)]\tLoss: 29.585280\n",
            "Train Epoch: 1 [10688/12611 (84%)]\tLoss: 27.592024\n",
            "Train Epoch: 1 [10752/12611 (85%)]\tLoss: 35.946725\n",
            "Train Epoch: 1 [10816/12611 (85%)]\tLoss: 33.065636\n",
            "Train Epoch: 1 [10880/12611 (86%)]\tLoss: 31.708894\n",
            "Train Epoch: 1 [10944/12611 (86%)]\tLoss: 28.687662\n",
            "Train Epoch: 1 [11008/12611 (87%)]\tLoss: 26.421949\n",
            "Train Epoch: 1 [11072/12611 (87%)]\tLoss: 29.348114\n",
            "Train Epoch: 1 [11136/12611 (88%)]\tLoss: 27.504024\n",
            "Train Epoch: 1 [11200/12611 (88%)]\tLoss: 27.732067\n",
            "Train Epoch: 1 [11264/12611 (89%)]\tLoss: 32.566868\n",
            "Train Epoch: 1 [11328/12611 (89%)]\tLoss: 29.710307\n",
            "Train Epoch: 1 [11392/12611 (90%)]\tLoss: 32.583384\n",
            "Train Epoch: 1 [11456/12611 (90%)]\tLoss: 28.533018\n",
            "Train Epoch: 1 [11520/12611 (91%)]\tLoss: 31.397683\n",
            "Train Epoch: 1 [11584/12611 (91%)]\tLoss: 28.141913\n",
            "Train Epoch: 1 [11648/12611 (92%)]\tLoss: 28.041202\n",
            "Train Epoch: 1 [11712/12611 (92%)]\tLoss: 27.992384\n",
            "Train Epoch: 1 [11776/12611 (93%)]\tLoss: 31.017631\n",
            "Train Epoch: 1 [11840/12611 (93%)]\tLoss: 23.198934\n",
            "Train Epoch: 1 [11904/12611 (94%)]\tLoss: 30.637721\n",
            "Train Epoch: 1 [11968/12611 (94%)]\tLoss: 29.699371\n",
            "Train Epoch: 1 [12032/12611 (95%)]\tLoss: 29.925694\n",
            "Train Epoch: 1 [12096/12611 (95%)]\tLoss: 28.376111\n",
            "Train Epoch: 1 [12160/12611 (96%)]\tLoss: 31.458055\n",
            "Train Epoch: 1 [12224/12611 (96%)]\tLoss: 33.252960\n",
            "Train Epoch: 1 [12288/12611 (97%)]\tLoss: 30.285632\n",
            "Train Epoch: 1 [12352/12611 (97%)]\tLoss: 26.551311\n",
            "Train Epoch: 1 [12416/12611 (98%)]\tLoss: 31.969054\n",
            "Train Epoch: 1 [12480/12611 (98%)]\tLoss: 28.690838\n",
            "Train Epoch: 1 [12544/12611 (99%)]\tLoss: 27.330762\n",
            "Train Epoch: 1 [591/12611 (99%)]\tLoss: 10.001562\n",
            "\n",
            " Train set: Average loss: 30.9400\n",
            "\n",
            "\n",
            " Test set: Average loss: 33.3028\n",
            "\n",
            "Train Epoch: 2 [0/12611 (0%)]\tLoss: 27.039712\n",
            "Train Epoch: 2 [64/12611 (1%)]\tLoss: 27.479204\n",
            "Train Epoch: 2 [128/12611 (1%)]\tLoss: 27.289441\n",
            "Train Epoch: 2 [192/12611 (2%)]\tLoss: 30.250820\n",
            "Train Epoch: 2 [256/12611 (2%)]\tLoss: 29.807077\n",
            "Train Epoch: 2 [320/12611 (3%)]\tLoss: 26.642959\n",
            "Train Epoch: 2 [384/12611 (3%)]\tLoss: 30.249495\n",
            "Train Epoch: 2 [448/12611 (4%)]\tLoss: 28.773281\n",
            "Train Epoch: 2 [512/12611 (4%)]\tLoss: 34.724291\n",
            "Train Epoch: 2 [576/12611 (5%)]\tLoss: 31.856785\n",
            "Train Epoch: 2 [640/12611 (5%)]\tLoss: 31.263575\n",
            "Train Epoch: 2 [704/12611 (6%)]\tLoss: 31.245096\n",
            "Train Epoch: 2 [768/12611 (6%)]\tLoss: 32.912144\n",
            "Train Epoch: 2 [832/12611 (7%)]\tLoss: 25.671613\n",
            "Train Epoch: 2 [896/12611 (7%)]\tLoss: 21.151872\n",
            "Train Epoch: 2 [960/12611 (8%)]\tLoss: 29.878707\n",
            "Train Epoch: 2 [1024/12611 (8%)]\tLoss: 28.425279\n",
            "Train Epoch: 2 [1088/12611 (9%)]\tLoss: 31.030888\n",
            "Train Epoch: 2 [1152/12611 (9%)]\tLoss: 26.726677\n",
            "Train Epoch: 2 [1216/12611 (10%)]\tLoss: 30.068180\n",
            "Train Epoch: 2 [1280/12611 (10%)]\tLoss: 29.777301\n",
            "Train Epoch: 2 [1344/12611 (11%)]\tLoss: 25.697065\n",
            "Train Epoch: 2 [1408/12611 (11%)]\tLoss: 31.058965\n",
            "Train Epoch: 2 [1472/12611 (12%)]\tLoss: 29.697340\n",
            "Train Epoch: 2 [1536/12611 (12%)]\tLoss: 23.083783\n",
            "Train Epoch: 2 [1600/12611 (13%)]\tLoss: 30.542966\n",
            "Train Epoch: 2 [1664/12611 (13%)]\tLoss: 26.497249\n",
            "Train Epoch: 2 [1728/12611 (14%)]\tLoss: 25.950174\n",
            "Train Epoch: 2 [1792/12611 (14%)]\tLoss: 26.785236\n",
            "Train Epoch: 2 [1856/12611 (15%)]\tLoss: 30.214729\n",
            "Train Epoch: 2 [1920/12611 (15%)]\tLoss: 32.263661\n",
            "Train Epoch: 2 [1984/12611 (16%)]\tLoss: 30.936452\n",
            "Train Epoch: 2 [2048/12611 (16%)]\tLoss: 22.082358\n",
            "Train Epoch: 2 [2112/12611 (17%)]\tLoss: 32.137880\n",
            "Train Epoch: 2 [2176/12611 (17%)]\tLoss: 25.474632\n",
            "Train Epoch: 2 [2240/12611 (18%)]\tLoss: 30.430502\n",
            "Train Epoch: 2 [2304/12611 (18%)]\tLoss: 32.626134\n",
            "Train Epoch: 2 [2368/12611 (19%)]\tLoss: 24.781381\n",
            "Train Epoch: 2 [2432/12611 (19%)]\tLoss: 28.658050\n",
            "Train Epoch: 2 [2496/12611 (20%)]\tLoss: 28.311963\n",
            "Train Epoch: 2 [2560/12611 (20%)]\tLoss: 30.385338\n",
            "Train Epoch: 2 [2624/12611 (21%)]\tLoss: 27.906750\n",
            "Train Epoch: 2 [2688/12611 (21%)]\tLoss: 29.590586\n",
            "Train Epoch: 2 [2752/12611 (22%)]\tLoss: 31.648681\n",
            "Train Epoch: 2 [2816/12611 (22%)]\tLoss: 32.044818\n",
            "Train Epoch: 2 [2880/12611 (23%)]\tLoss: 29.340161\n",
            "Train Epoch: 2 [2944/12611 (23%)]\tLoss: 28.972110\n",
            "Train Epoch: 2 [3008/12611 (24%)]\tLoss: 28.672004\n",
            "Train Epoch: 2 [3072/12611 (24%)]\tLoss: 29.007244\n",
            "Train Epoch: 2 [3136/12611 (25%)]\tLoss: 23.078562\n",
            "Train Epoch: 2 [3200/12611 (25%)]\tLoss: 24.428582\n",
            "Train Epoch: 2 [3264/12611 (26%)]\tLoss: 28.590195\n",
            "Train Epoch: 2 [3328/12611 (26%)]\tLoss: 26.001441\n",
            "Train Epoch: 2 [3392/12611 (27%)]\tLoss: 27.529717\n",
            "Train Epoch: 2 [3456/12611 (27%)]\tLoss: 29.793679\n",
            "Train Epoch: 2 [3520/12611 (28%)]\tLoss: 32.533937\n",
            "Train Epoch: 2 [3584/12611 (28%)]\tLoss: 26.554602\n",
            "Train Epoch: 2 [3648/12611 (29%)]\tLoss: 28.885190\n",
            "Train Epoch: 2 [3712/12611 (29%)]\tLoss: 30.264161\n",
            "Train Epoch: 2 [3776/12611 (30%)]\tLoss: 30.951655\n",
            "Train Epoch: 2 [3840/12611 (30%)]\tLoss: 28.140088\n",
            "Train Epoch: 2 [3904/12611 (31%)]\tLoss: 25.786890\n",
            "Train Epoch: 2 [3968/12611 (31%)]\tLoss: 25.741792\n",
            "Train Epoch: 2 [4032/12611 (32%)]\tLoss: 25.946161\n",
            "Train Epoch: 2 [4096/12611 (32%)]\tLoss: 28.896472\n",
            "Train Epoch: 2 [4160/12611 (33%)]\tLoss: 25.696683\n",
            "Train Epoch: 2 [4224/12611 (33%)]\tLoss: 29.269300\n",
            "Train Epoch: 2 [4288/12611 (34%)]\tLoss: 33.487757\n",
            "Train Epoch: 2 [4352/12611 (34%)]\tLoss: 26.508408\n",
            "Train Epoch: 2 [4416/12611 (35%)]\tLoss: 24.889471\n",
            "Train Epoch: 2 [4480/12611 (35%)]\tLoss: 28.293465\n",
            "Train Epoch: 2 [4544/12611 (36%)]\tLoss: 27.341328\n",
            "Train Epoch: 2 [4608/12611 (36%)]\tLoss: 27.899156\n",
            "Train Epoch: 2 [4672/12611 (37%)]\tLoss: 29.648436\n",
            "Train Epoch: 2 [4736/12611 (37%)]\tLoss: 24.245264\n",
            "Train Epoch: 2 [4800/12611 (38%)]\tLoss: 29.949672\n",
            "Train Epoch: 2 [4864/12611 (38%)]\tLoss: 26.495410\n",
            "Train Epoch: 2 [4928/12611 (39%)]\tLoss: 26.303114\n",
            "Train Epoch: 2 [4992/12611 (39%)]\tLoss: 27.835486\n",
            "Train Epoch: 2 [5056/12611 (40%)]\tLoss: 24.452015\n",
            "Train Epoch: 2 [5120/12611 (40%)]\tLoss: 28.770297\n",
            "Train Epoch: 2 [5184/12611 (41%)]\tLoss: 28.010700\n",
            "Train Epoch: 2 [5248/12611 (41%)]\tLoss: 28.875718\n",
            "Train Epoch: 2 [5312/12611 (42%)]\tLoss: 30.568139\n",
            "Train Epoch: 2 [5376/12611 (42%)]\tLoss: 30.763275\n",
            "Train Epoch: 2 [5440/12611 (43%)]\tLoss: 28.618567\n",
            "Train Epoch: 2 [5504/12611 (43%)]\tLoss: 29.198547\n",
            "Train Epoch: 2 [5568/12611 (44%)]\tLoss: 27.835034\n",
            "Train Epoch: 2 [5632/12611 (44%)]\tLoss: 31.618111\n",
            "Train Epoch: 2 [5696/12611 (45%)]\tLoss: 29.340230\n",
            "Train Epoch: 2 [5760/12611 (45%)]\tLoss: 26.442039\n",
            "Train Epoch: 2 [5824/12611 (46%)]\tLoss: 25.927277\n",
            "Train Epoch: 2 [5888/12611 (46%)]\tLoss: 23.262539\n",
            "Train Epoch: 2 [5952/12611 (47%)]\tLoss: 32.013245\n",
            "Train Epoch: 2 [6016/12611 (47%)]\tLoss: 29.541125\n",
            "Train Epoch: 2 [6080/12611 (48%)]\tLoss: 24.410157\n",
            "Train Epoch: 2 [6144/12611 (48%)]\tLoss: 28.643846\n",
            "Train Epoch: 2 [6208/12611 (49%)]\tLoss: 31.830123\n",
            "Train Epoch: 2 [6272/12611 (49%)]\tLoss: 23.717617\n",
            "Train Epoch: 2 [6336/12611 (50%)]\tLoss: 29.425630\n",
            "Train Epoch: 2 [6400/12611 (51%)]\tLoss: 27.174255\n",
            "Train Epoch: 2 [6464/12611 (51%)]\tLoss: 24.293756\n",
            "Train Epoch: 2 [6528/12611 (52%)]\tLoss: 21.955360\n",
            "Train Epoch: 2 [6592/12611 (52%)]\tLoss: 25.747167\n",
            "Train Epoch: 2 [6656/12611 (53%)]\tLoss: 27.375559\n",
            "Train Epoch: 2 [6720/12611 (53%)]\tLoss: 32.425863\n",
            "Train Epoch: 2 [6784/12611 (54%)]\tLoss: 28.194976\n",
            "Train Epoch: 2 [6848/12611 (54%)]\tLoss: 29.913598\n",
            "Train Epoch: 2 [6912/12611 (55%)]\tLoss: 28.040785\n",
            "Train Epoch: 2 [6976/12611 (55%)]\tLoss: 28.179266\n",
            "Train Epoch: 2 [7040/12611 (56%)]\tLoss: 25.761894\n",
            "Train Epoch: 2 [7104/12611 (56%)]\tLoss: 27.520550\n",
            "Train Epoch: 2 [7168/12611 (57%)]\tLoss: 27.051909\n",
            "Train Epoch: 2 [7232/12611 (57%)]\tLoss: 28.989289\n",
            "Train Epoch: 2 [7296/12611 (58%)]\tLoss: 23.802907\n",
            "Train Epoch: 2 [7360/12611 (58%)]\tLoss: 26.047493\n",
            "Train Epoch: 2 [7424/12611 (59%)]\tLoss: 26.743752\n",
            "Train Epoch: 2 [7488/12611 (59%)]\tLoss: 27.420148\n",
            "Train Epoch: 2 [7552/12611 (60%)]\tLoss: 28.681597\n",
            "Train Epoch: 2 [7616/12611 (60%)]\tLoss: 28.934689\n",
            "Train Epoch: 2 [7680/12611 (61%)]\tLoss: 26.333632\n",
            "Train Epoch: 2 [7744/12611 (61%)]\tLoss: 22.909086\n",
            "Train Epoch: 2 [7808/12611 (62%)]\tLoss: 27.261406\n",
            "Train Epoch: 2 [7872/12611 (62%)]\tLoss: 28.114812\n",
            "Train Epoch: 2 [7936/12611 (63%)]\tLoss: 25.572003\n",
            "Train Epoch: 2 [8000/12611 (63%)]\tLoss: 28.789952\n",
            "Train Epoch: 2 [8064/12611 (64%)]\tLoss: 29.082800\n",
            "Train Epoch: 2 [8128/12611 (64%)]\tLoss: 27.779888\n",
            "Train Epoch: 2 [8192/12611 (65%)]\tLoss: 28.747653\n",
            "Train Epoch: 2 [8256/12611 (65%)]\tLoss: 31.064490\n",
            "Train Epoch: 2 [8320/12611 (66%)]\tLoss: 28.745218\n",
            "Train Epoch: 2 [8384/12611 (66%)]\tLoss: 31.953276\n",
            "Train Epoch: 2 [8448/12611 (67%)]\tLoss: 27.328868\n",
            "Train Epoch: 2 [8512/12611 (67%)]\tLoss: 26.335938\n",
            "Train Epoch: 2 [8576/12611 (68%)]\tLoss: 28.328596\n",
            "Train Epoch: 2 [8640/12611 (68%)]\tLoss: 28.913378\n",
            "Train Epoch: 2 [8704/12611 (69%)]\tLoss: 25.507411\n",
            "Train Epoch: 2 [8768/12611 (69%)]\tLoss: 23.969687\n",
            "Train Epoch: 2 [8832/12611 (70%)]\tLoss: 27.484041\n",
            "Train Epoch: 2 [8896/12611 (70%)]\tLoss: 27.288322\n",
            "Train Epoch: 2 [8960/12611 (71%)]\tLoss: 24.825946\n",
            "Train Epoch: 2 [9024/12611 (71%)]\tLoss: 31.247364\n",
            "Train Epoch: 2 [9088/12611 (72%)]\tLoss: 28.822752\n",
            "Train Epoch: 2 [9152/12611 (72%)]\tLoss: 31.281753\n",
            "Train Epoch: 2 [9216/12611 (73%)]\tLoss: 25.363232\n",
            "Train Epoch: 2 [9280/12611 (73%)]\tLoss: 29.134303\n",
            "Train Epoch: 2 [9408/12611 (74%)]\tLoss: 29.376534\n",
            "Train Epoch: 2 [9472/12611 (75%)]\tLoss: 25.595317\n",
            "Train Epoch: 2 [9536/12611 (75%)]\tLoss: 29.251514\n",
            "Train Epoch: 2 [9600/12611 (76%)]\tLoss: 26.430457\n",
            "Train Epoch: 2 [9664/12611 (76%)]\tLoss: 23.571824\n",
            "Train Epoch: 2 [9728/12611 (77%)]\tLoss: 28.789110\n",
            "Train Epoch: 2 [9792/12611 (77%)]\tLoss: 25.092591\n",
            "Train Epoch: 2 [9856/12611 (78%)]\tLoss: 22.907424\n",
            "Train Epoch: 2 [9920/12611 (78%)]\tLoss: 26.394732\n",
            "Train Epoch: 2 [9984/12611 (79%)]\tLoss: 27.193674\n",
            "Train Epoch: 2 [10048/12611 (79%)]\tLoss: 28.746790\n",
            "Train Epoch: 2 [10112/12611 (80%)]\tLoss: 27.727245\n",
            "Train Epoch: 2 [10176/12611 (80%)]\tLoss: 24.602643\n",
            "Train Epoch: 2 [10240/12611 (81%)]\tLoss: 28.888739\n",
            "Train Epoch: 2 [10304/12611 (81%)]\tLoss: 27.643327\n",
            "Train Epoch: 2 [10368/12611 (82%)]\tLoss: 21.626326\n",
            "Train Epoch: 2 [10432/12611 (82%)]\tLoss: 29.720773\n",
            "Train Epoch: 2 [10496/12611 (83%)]\tLoss: 27.357866\n",
            "Train Epoch: 2 [10560/12611 (83%)]\tLoss: 26.429706\n",
            "Train Epoch: 2 [10624/12611 (84%)]\tLoss: 28.726864\n",
            "Train Epoch: 2 [10688/12611 (84%)]\tLoss: 24.830248\n",
            "Train Epoch: 2 [10752/12611 (85%)]\tLoss: 28.707535\n",
            "Train Epoch: 2 [10816/12611 (85%)]\tLoss: 28.584974\n",
            "Train Epoch: 2 [10880/12611 (86%)]\tLoss: 27.175446\n",
            "Train Epoch: 2 [10944/12611 (86%)]\tLoss: 27.616789\n",
            "Train Epoch: 2 [11008/12611 (87%)]\tLoss: 21.716523\n",
            "Train Epoch: 2 [11072/12611 (87%)]\tLoss: 24.763277\n",
            "Train Epoch: 2 [11136/12611 (88%)]\tLoss: 28.983386\n",
            "Train Epoch: 2 [11200/12611 (88%)]\tLoss: 25.303653\n",
            "Train Epoch: 2 [11264/12611 (89%)]\tLoss: 28.713321\n",
            "Train Epoch: 2 [11328/12611 (89%)]\tLoss: 25.907447\n",
            "Train Epoch: 2 [11392/12611 (90%)]\tLoss: 30.233931\n",
            "Train Epoch: 2 [11456/12611 (90%)]\tLoss: 31.324559\n",
            "Train Epoch: 2 [11520/12611 (91%)]\tLoss: 22.877714\n",
            "Train Epoch: 2 [11584/12611 (91%)]\tLoss: 23.465809\n",
            "Train Epoch: 2 [11648/12611 (92%)]\tLoss: 25.370204\n",
            "Train Epoch: 2 [11712/12611 (92%)]\tLoss: 24.050530\n",
            "Train Epoch: 2 [11776/12611 (93%)]\tLoss: 28.751710\n",
            "Train Epoch: 2 [11840/12611 (93%)]\tLoss: 27.506120\n",
            "Train Epoch: 2 [11904/12611 (94%)]\tLoss: 26.098067\n",
            "Train Epoch: 2 [11968/12611 (94%)]\tLoss: 23.724442\n",
            "Train Epoch: 2 [12032/12611 (95%)]\tLoss: 32.182395\n",
            "Train Epoch: 2 [12096/12611 (95%)]\tLoss: 26.313806\n",
            "Train Epoch: 2 [12160/12611 (96%)]\tLoss: 28.569028\n",
            "Train Epoch: 2 [12224/12611 (96%)]\tLoss: 27.079082\n",
            "Train Epoch: 2 [12288/12611 (97%)]\tLoss: 24.404795\n",
            "Train Epoch: 2 [12352/12611 (97%)]\tLoss: 28.464487\n",
            "Train Epoch: 2 [12416/12611 (98%)]\tLoss: 28.747281\n",
            "Train Epoch: 2 [12480/12611 (98%)]\tLoss: 31.143231\n",
            "Train Epoch: 2 [12544/12611 (99%)]\tLoss: 30.800412\n",
            "Train Epoch: 2 [591/12611 (99%)]\tLoss: 10.036842\n",
            "\n",
            " Train set: Average loss: 25.7184\n",
            "\n",
            "\n",
            " Test set: Average loss: 31.2926\n",
            "\n",
            "Train Epoch: 3 [0/12611 (0%)]\tLoss: 21.422783\n",
            "Train Epoch: 3 [64/12611 (1%)]\tLoss: 30.330656\n",
            "Train Epoch: 3 [128/12611 (1%)]\tLoss: 25.316901\n",
            "Train Epoch: 3 [192/12611 (2%)]\tLoss: 23.539646\n",
            "Train Epoch: 3 [256/12611 (2%)]\tLoss: 25.608967\n",
            "Train Epoch: 3 [320/12611 (3%)]\tLoss: 34.999086\n",
            "Train Epoch: 3 [384/12611 (3%)]\tLoss: 21.410501\n",
            "Train Epoch: 3 [448/12611 (4%)]\tLoss: 20.469145\n",
            "Train Epoch: 3 [512/12611 (4%)]\tLoss: 22.860766\n",
            "Train Epoch: 3 [576/12611 (5%)]\tLoss: 24.983596\n",
            "Train Epoch: 3 [640/12611 (5%)]\tLoss: 28.595705\n",
            "Train Epoch: 3 [704/12611 (6%)]\tLoss: 29.103336\n",
            "Train Epoch: 3 [768/12611 (6%)]\tLoss: 27.213970\n",
            "Train Epoch: 3 [832/12611 (7%)]\tLoss: 21.667257\n",
            "Train Epoch: 3 [896/12611 (7%)]\tLoss: 23.358908\n",
            "Train Epoch: 3 [960/12611 (8%)]\tLoss: 25.701988\n",
            "Train Epoch: 3 [1024/12611 (8%)]\tLoss: 26.379132\n",
            "Train Epoch: 3 [1088/12611 (9%)]\tLoss: 27.951390\n",
            "Train Epoch: 3 [1152/12611 (9%)]\tLoss: 26.121763\n",
            "Train Epoch: 3 [1216/12611 (10%)]\tLoss: 32.347572\n",
            "Train Epoch: 3 [1280/12611 (10%)]\tLoss: 25.739291\n",
            "Train Epoch: 3 [1344/12611 (11%)]\tLoss: 26.348591\n",
            "Train Epoch: 3 [1408/12611 (11%)]\tLoss: 31.512693\n",
            "Train Epoch: 3 [1472/12611 (12%)]\tLoss: 26.543567\n",
            "Train Epoch: 3 [1536/12611 (12%)]\tLoss: 22.775838\n",
            "Train Epoch: 3 [1600/12611 (13%)]\tLoss: 27.165650\n",
            "Train Epoch: 3 [1664/12611 (13%)]\tLoss: 23.620954\n",
            "Train Epoch: 3 [1728/12611 (14%)]\tLoss: 22.247107\n",
            "Train Epoch: 3 [1792/12611 (14%)]\tLoss: 25.549838\n",
            "Train Epoch: 3 [1856/12611 (15%)]\tLoss: 27.886502\n",
            "Train Epoch: 3 [1920/12611 (15%)]\tLoss: 24.163365\n",
            "Train Epoch: 3 [1984/12611 (16%)]\tLoss: 24.079886\n",
            "Train Epoch: 3 [2048/12611 (16%)]\tLoss: 27.637219\n",
            "Train Epoch: 3 [2112/12611 (17%)]\tLoss: 24.769841\n",
            "Train Epoch: 3 [2176/12611 (17%)]\tLoss: 23.446529\n",
            "Train Epoch: 3 [2240/12611 (18%)]\tLoss: 20.948091\n",
            "Train Epoch: 3 [2304/12611 (18%)]\tLoss: 24.153032\n",
            "Train Epoch: 3 [2368/12611 (19%)]\tLoss: 20.442206\n",
            "Train Epoch: 3 [2432/12611 (19%)]\tLoss: 23.125979\n",
            "Train Epoch: 3 [2496/12611 (20%)]\tLoss: 23.508280\n",
            "Train Epoch: 3 [2560/12611 (20%)]\tLoss: 23.150762\n",
            "Train Epoch: 3 [2624/12611 (21%)]\tLoss: 20.454217\n",
            "Train Epoch: 3 [2688/12611 (21%)]\tLoss: 26.378087\n",
            "Train Epoch: 3 [2752/12611 (22%)]\tLoss: 24.418959\n",
            "Train Epoch: 3 [2816/12611 (22%)]\tLoss: 25.629053\n",
            "Train Epoch: 3 [2880/12611 (23%)]\tLoss: 27.679392\n",
            "Train Epoch: 3 [2944/12611 (23%)]\tLoss: 27.837929\n",
            "Train Epoch: 3 [3008/12611 (24%)]\tLoss: 28.381614\n",
            "Train Epoch: 3 [3072/12611 (24%)]\tLoss: 27.010736\n",
            "Train Epoch: 3 [3136/12611 (25%)]\tLoss: 23.896257\n",
            "Train Epoch: 3 [3200/12611 (25%)]\tLoss: 25.845328\n",
            "Train Epoch: 3 [3264/12611 (26%)]\tLoss: 23.023059\n",
            "Train Epoch: 3 [3328/12611 (26%)]\tLoss: 28.332715\n",
            "Train Epoch: 3 [3392/12611 (27%)]\tLoss: 27.601227\n",
            "Train Epoch: 3 [3456/12611 (27%)]\tLoss: 26.453284\n",
            "Train Epoch: 3 [3520/12611 (28%)]\tLoss: 25.809546\n",
            "Train Epoch: 3 [3584/12611 (28%)]\tLoss: 27.243714\n",
            "Train Epoch: 3 [3648/12611 (29%)]\tLoss: 25.552965\n",
            "Train Epoch: 3 [3712/12611 (29%)]\tLoss: 26.358330\n",
            "Train Epoch: 3 [3776/12611 (30%)]\tLoss: 24.110193\n",
            "Train Epoch: 3 [3840/12611 (30%)]\tLoss: 28.098618\n",
            "Train Epoch: 3 [3904/12611 (31%)]\tLoss: 26.341918\n",
            "Train Epoch: 3 [3968/12611 (31%)]\tLoss: 24.123319\n",
            "Train Epoch: 3 [4032/12611 (32%)]\tLoss: 23.437808\n",
            "Train Epoch: 3 [4096/12611 (32%)]\tLoss: 22.193185\n",
            "Train Epoch: 3 [4160/12611 (33%)]\tLoss: 25.130018\n",
            "Train Epoch: 3 [4224/12611 (33%)]\tLoss: 25.908137\n",
            "Train Epoch: 3 [4288/12611 (34%)]\tLoss: 26.318376\n",
            "Train Epoch: 3 [4352/12611 (34%)]\tLoss: 27.711162\n",
            "Train Epoch: 3 [4416/12611 (35%)]\tLoss: 28.438137\n",
            "Train Epoch: 3 [4480/12611 (35%)]\tLoss: 23.999799\n",
            "Train Epoch: 3 [4544/12611 (36%)]\tLoss: 30.314572\n",
            "Train Epoch: 3 [4608/12611 (36%)]\tLoss: 26.606254\n",
            "Train Epoch: 3 [4672/12611 (37%)]\tLoss: 26.551112\n",
            "Train Epoch: 3 [4736/12611 (37%)]\tLoss: 23.483313\n",
            "Train Epoch: 3 [4800/12611 (38%)]\tLoss: 25.042500\n",
            "Train Epoch: 3 [4864/12611 (38%)]\tLoss: 25.916615\n",
            "Train Epoch: 3 [4928/12611 (39%)]\tLoss: 26.206947\n",
            "Train Epoch: 3 [4992/12611 (39%)]\tLoss: 32.997933\n",
            "Train Epoch: 3 [5056/12611 (40%)]\tLoss: 24.656765\n",
            "Train Epoch: 3 [5120/12611 (40%)]\tLoss: 24.218322\n",
            "Train Epoch: 3 [5184/12611 (41%)]\tLoss: 25.488436\n",
            "Train Epoch: 3 [5248/12611 (41%)]\tLoss: 20.070296\n",
            "Train Epoch: 3 [5312/12611 (42%)]\tLoss: 24.682243\n",
            "Train Epoch: 3 [5376/12611 (42%)]\tLoss: 27.882787\n",
            "Train Epoch: 3 [5440/12611 (43%)]\tLoss: 28.713741\n",
            "Train Epoch: 3 [5504/12611 (43%)]\tLoss: 24.055470\n",
            "Train Epoch: 3 [5568/12611 (44%)]\tLoss: 22.868622\n",
            "Train Epoch: 3 [5632/12611 (44%)]\tLoss: 25.966766\n",
            "Train Epoch: 3 [5696/12611 (45%)]\tLoss: 24.895126\n",
            "Train Epoch: 3 [5760/12611 (45%)]\tLoss: 28.752111\n",
            "Train Epoch: 3 [5824/12611 (46%)]\tLoss: 25.053416\n",
            "Train Epoch: 3 [5888/12611 (46%)]\tLoss: 26.116596\n",
            "Train Epoch: 3 [5952/12611 (47%)]\tLoss: 22.928627\n",
            "Train Epoch: 3 [6016/12611 (47%)]\tLoss: 23.791290\n",
            "Train Epoch: 3 [6080/12611 (48%)]\tLoss: 25.463925\n",
            "Train Epoch: 3 [6144/12611 (48%)]\tLoss: 24.706645\n",
            "Train Epoch: 3 [6208/12611 (49%)]\tLoss: 27.746640\n",
            "Train Epoch: 3 [6272/12611 (49%)]\tLoss: 20.394404\n",
            "Train Epoch: 3 [6336/12611 (50%)]\tLoss: 22.467140\n",
            "Train Epoch: 3 [6400/12611 (51%)]\tLoss: 28.346715\n",
            "Train Epoch: 3 [6464/12611 (51%)]\tLoss: 22.598300\n",
            "Train Epoch: 3 [6528/12611 (52%)]\tLoss: 26.306047\n",
            "Train Epoch: 3 [6592/12611 (52%)]\tLoss: 22.198839\n",
            "Train Epoch: 3 [6656/12611 (53%)]\tLoss: 26.400960\n",
            "Train Epoch: 3 [6720/12611 (53%)]\tLoss: 29.311907\n",
            "Train Epoch: 3 [6784/12611 (54%)]\tLoss: 29.913399\n",
            "Train Epoch: 3 [6848/12611 (54%)]\tLoss: 21.805270\n",
            "Train Epoch: 3 [6912/12611 (55%)]\tLoss: 27.766947\n",
            "Train Epoch: 3 [6976/12611 (55%)]\tLoss: 24.059229\n",
            "Train Epoch: 3 [7040/12611 (56%)]\tLoss: 22.860374\n",
            "Train Epoch: 3 [7104/12611 (56%)]\tLoss: 27.399497\n",
            "Train Epoch: 3 [7168/12611 (57%)]\tLoss: 26.441146\n",
            "Train Epoch: 3 [7232/12611 (57%)]\tLoss: 25.608306\n",
            "Train Epoch: 3 [7296/12611 (58%)]\tLoss: 24.616296\n",
            "Train Epoch: 3 [7360/12611 (58%)]\tLoss: 25.419057\n",
            "Train Epoch: 3 [7424/12611 (59%)]\tLoss: 26.752578\n",
            "Train Epoch: 3 [7488/12611 (59%)]\tLoss: 24.576616\n",
            "Train Epoch: 3 [7552/12611 (60%)]\tLoss: 23.011712\n",
            "Train Epoch: 3 [7616/12611 (60%)]\tLoss: 26.530129\n",
            "Train Epoch: 3 [7680/12611 (61%)]\tLoss: 25.611481\n",
            "Train Epoch: 3 [7744/12611 (61%)]\tLoss: 28.664337\n",
            "Train Epoch: 3 [7808/12611 (62%)]\tLoss: 22.756720\n",
            "Train Epoch: 3 [7872/12611 (62%)]\tLoss: 24.568555\n",
            "Train Epoch: 3 [7936/12611 (63%)]\tLoss: 24.884177\n",
            "Train Epoch: 3 [8000/12611 (63%)]\tLoss: 22.209540\n",
            "Train Epoch: 3 [8064/12611 (64%)]\tLoss: 24.910108\n",
            "Train Epoch: 3 [8128/12611 (64%)]\tLoss: 24.231768\n",
            "Train Epoch: 3 [8192/12611 (65%)]\tLoss: 25.100640\n",
            "Train Epoch: 3 [8256/12611 (65%)]\tLoss: 22.371863\n",
            "Train Epoch: 3 [8320/12611 (66%)]\tLoss: 31.028180\n",
            "Train Epoch: 3 [8384/12611 (66%)]\tLoss: 27.152145\n",
            "Train Epoch: 3 [8448/12611 (67%)]\tLoss: 22.043769\n",
            "Train Epoch: 3 [8512/12611 (67%)]\tLoss: 25.323733\n",
            "Train Epoch: 3 [8576/12611 (68%)]\tLoss: 24.169238\n",
            "Train Epoch: 3 [8640/12611 (68%)]\tLoss: 24.779302\n",
            "Train Epoch: 3 [8704/12611 (69%)]\tLoss: 22.640598\n",
            "Train Epoch: 3 [8768/12611 (69%)]\tLoss: 22.392590\n",
            "Train Epoch: 3 [8832/12611 (70%)]\tLoss: 26.231766\n",
            "Train Epoch: 3 [8896/12611 (70%)]\tLoss: 19.399464\n",
            "Train Epoch: 3 [8960/12611 (71%)]\tLoss: 20.071462\n",
            "Train Epoch: 3 [9024/12611 (71%)]\tLoss: 23.891689\n",
            "Train Epoch: 3 [9088/12611 (72%)]\tLoss: 27.141296\n",
            "Train Epoch: 3 [9152/12611 (72%)]\tLoss: 24.606919\n",
            "Train Epoch: 3 [9216/12611 (73%)]\tLoss: 27.466499\n",
            "Train Epoch: 3 [9280/12611 (73%)]\tLoss: 20.133956\n",
            "Train Epoch: 3 [9344/12611 (74%)]\tLoss: 21.917013\n",
            "Train Epoch: 3 [9408/12611 (74%)]\tLoss: 27.821318\n",
            "Train Epoch: 3 [9472/12611 (75%)]\tLoss: 30.077653\n",
            "Train Epoch: 3 [9536/12611 (75%)]\tLoss: 25.610704\n",
            "Train Epoch: 3 [9600/12611 (76%)]\tLoss: 25.453469\n",
            "Train Epoch: 3 [9664/12611 (76%)]\tLoss: 26.637716\n",
            "Train Epoch: 3 [9728/12611 (77%)]\tLoss: 26.124935\n",
            "Train Epoch: 3 [9792/12611 (77%)]\tLoss: 22.721889\n",
            "Train Epoch: 3 [9856/12611 (78%)]\tLoss: 26.663406\n",
            "Train Epoch: 3 [9920/12611 (78%)]\tLoss: 26.882270\n",
            "Train Epoch: 3 [9984/12611 (79%)]\tLoss: 22.073459\n",
            "Train Epoch: 3 [10048/12611 (79%)]\tLoss: 22.741506\n",
            "Train Epoch: 3 [10112/12611 (80%)]\tLoss: 20.360182\n",
            "Train Epoch: 3 [10176/12611 (80%)]\tLoss: 25.219311\n",
            "Train Epoch: 3 [10240/12611 (81%)]\tLoss: 23.477460\n",
            "Train Epoch: 3 [10304/12611 (81%)]\tLoss: 22.986122\n",
            "Train Epoch: 3 [10368/12611 (82%)]\tLoss: 22.761155\n",
            "Train Epoch: 3 [10432/12611 (82%)]\tLoss: 22.056710\n",
            "Train Epoch: 3 [10496/12611 (83%)]\tLoss: 24.558795\n",
            "Train Epoch: 3 [10560/12611 (83%)]\tLoss: 28.912873\n",
            "Train Epoch: 3 [10624/12611 (84%)]\tLoss: 25.911966\n",
            "Train Epoch: 3 [10688/12611 (84%)]\tLoss: 20.399230\n",
            "Train Epoch: 3 [10752/12611 (85%)]\tLoss: 22.639683\n",
            "Train Epoch: 3 [10816/12611 (85%)]\tLoss: 20.943836\n",
            "Train Epoch: 3 [10880/12611 (86%)]\tLoss: 25.979812\n",
            "Train Epoch: 3 [10944/12611 (86%)]\tLoss: 27.436019\n",
            "Train Epoch: 3 [11008/12611 (87%)]\tLoss: 28.378022\n",
            "Train Epoch: 3 [11072/12611 (87%)]\tLoss: 22.082391\n",
            "Train Epoch: 3 [11136/12611 (88%)]\tLoss: 25.508484\n",
            "Train Epoch: 3 [11200/12611 (88%)]\tLoss: 21.180242\n",
            "Train Epoch: 3 [11264/12611 (89%)]\tLoss: 23.043947\n",
            "Train Epoch: 3 [11328/12611 (89%)]\tLoss: 25.983932\n",
            "Train Epoch: 3 [11392/12611 (90%)]\tLoss: 25.712168\n",
            "Train Epoch: 3 [11456/12611 (90%)]\tLoss: 21.465266\n",
            "Train Epoch: 3 [11520/12611 (91%)]\tLoss: 22.334674\n",
            "Train Epoch: 3 [11584/12611 (91%)]\tLoss: 25.945802\n",
            "Train Epoch: 3 [11648/12611 (92%)]\tLoss: 26.313361\n",
            "Train Epoch: 3 [11712/12611 (92%)]\tLoss: 26.983548\n",
            "Train Epoch: 3 [11776/12611 (93%)]\tLoss: 27.230573\n",
            "Train Epoch: 3 [11840/12611 (93%)]\tLoss: 23.601856\n",
            "Train Epoch: 3 [11904/12611 (94%)]\tLoss: 24.565555\n",
            "Train Epoch: 3 [11968/12611 (94%)]\tLoss: 22.976197\n",
            "Train Epoch: 3 [12032/12611 (95%)]\tLoss: 22.841812\n",
            "Train Epoch: 3 [12096/12611 (95%)]\tLoss: 24.020329\n",
            "Train Epoch: 3 [12160/12611 (96%)]\tLoss: 23.195254\n",
            "Train Epoch: 3 [12224/12611 (96%)]\tLoss: 21.080488\n",
            "Train Epoch: 3 [12288/12611 (97%)]\tLoss: 23.470340\n",
            "Train Epoch: 3 [12352/12611 (97%)]\tLoss: 20.008653\n",
            "Train Epoch: 3 [12416/12611 (98%)]\tLoss: 20.677161\n",
            "Train Epoch: 3 [12480/12611 (98%)]\tLoss: 24.073405\n",
            "Train Epoch: 3 [12544/12611 (99%)]\tLoss: 24.214763\n",
            "Train Epoch: 3 [591/12611 (99%)]\tLoss: 36.160057\n",
            "\n",
            " Train set: Average loss: 25.3681\n",
            "\n",
            "\n",
            " Test set: Average loss: 30.5013\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check = torch.rand(5,1,256,256)\n",
        "# model = BoneAgePredictor();\n",
        "# print(model.forward(check).size())"
      ],
      "metadata": {
        "id": "JV1tLDbUCw0Z"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check = pd.read_csv(dataset_path('boneage-training-dataset.csv'));\n",
        "# print(check)\n",
        "# print(check['boneage'][0])\n",
        "# print(check['id'][1])\n",
        "# check = Image.open(dataset_path('boneage-training-dataset',str(check['id'][1])+'.png')).resize((256,256))\n",
        "# print(check)\n",
        "# # check = transform(check)\n",
        "# # print(check.size())\n",
        "# # check = torch.from_numpy(np.array(check['boneage'][0]))\n",
        "# # print(check)"
      ],
      "metadata": {
        "id": "W1zPxAL38T_w"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}