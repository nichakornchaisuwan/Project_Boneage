{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOChZ/CKDBUjVsrCRGsou/R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nichakornchaisuwan/Project_Boneage/blob/main/train_Bonet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9BesNpUgc1zD"
      },
      "outputs": [],
      "source": [
        "# Standard lib imports\n",
        "import os\n",
        "import csv\n",
        "import glob\n",
        "import time\n",
        "import argparse\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import os.path as osp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install horovod"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Zi5yOmgdABW",
        "outputId": "ae54b97c-7da9-4a9c-e439-5a95917a3ee9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting horovod\n",
            "  Downloading horovod-0.26.1.tar.gz (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 30.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cloudpickle in /usr/local/lib/python3.8/dist-packages (from horovod) (1.5.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from horovod) (5.4.8)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from horovod) (6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from horovod) (21.3)\n",
            "Requirement already satisfied: cffi>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from horovod) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.4.0->horovod) (2.21)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->horovod) (3.0.9)\n",
            "Building wheels for collected packages: horovod\n",
            "  Building wheel for horovod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for horovod: filename=horovod-0.26.1-cp38-cp38-linux_x86_64.whl size=28369052 sha256=8e764b9ee933700dd1f6dd86f90e3b79fd67744376c79010ff916ac6f82d02f5\n",
            "  Stored in directory: /root/.cache/pip/wheels/eb/0b/90/d53058f75f3ae3db9557f3e55dd8c016b2397e9b38557c8b66\n",
            "Successfully built horovod\n",
            "Installing collected packages: horovod\n",
            "Successfully installed horovod-0.26.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/BCV-Uniandes/Bonet.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cCdWIPzc_90",
        "outputId": "b2f3c26f-8045-4e50-9d16-db7bce2472d8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Bonet'...\n",
            "remote: Enumerating objects: 52, done.\u001b[K\n",
            "remote: Total 52 (delta 0), reused 0 (delta 0), pack-reused 52\u001b[K\n",
            "Unpacking objects: 100% (52/52), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd Bonet\n"
      ],
      "metadata": {
        "id": "Q3SKqDkdc_7G"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import horovod.torch as hvd\n",
        "from torchvision import transforms\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.distributed import DistributedSampler"
      ],
      "metadata": {
        "id": "qO-4yIxFc_4h"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Other imports\n",
        "from tqdm import tqdm\n",
        "import pdb"
      ],
      "metadata": {
        "id": "8k4xDOkVc_1v"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import Bonet\n",
        "from Bonet.models.bonet import BoNet"
      ],
      "metadata": {
        "id": "5lTqtkkQYy2e"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!/bin/bash train_net.sh"
      ],
      "metadata": {
        "id": "5maLdtM8Afub",
        "outputId": "66b9f8a3-39a4-4737-ad4f-de46bdf96141",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: train_net.sh: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import data-train\n",
        "from Bonet.train_net import mkdir"
      ],
      "metadata": {
        "id": "kju1zUrYf2J9",
        "outputId": "3115399b-3a8d-4f0f-c9a1-67a120a8ec58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-b3426293eff0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#import data-train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mBonet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVE_FOLDER\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmkdir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Bonet.train_net'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "B8wtXDfZc_y1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser()"
      ],
      "metadata": {
        "id": "tH0S-hWIc_wK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataloading-related settings\n",
        "parser.add_argument('--heatmaps', default=False, action='store_true',\n",
        "                help='Train model with gaussian heatmaps')\n",
        "parser.add_argument('--cropped', default=False, action='store_true',\n",
        "                help='Train model with cropped images according to bbox')\n",
        "parser.add_argument('--dataset', default='RSNA', type=str,choices=['RSNA','RHPE'],\n",
        "                help='Dataset to perform training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0R86MyVdee_",
        "outputId": "0b1ba6af-48ad-4187-a3ae-59b2223791a9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['--dataset'], dest='dataset', nargs=None, const=None, default='RSNA', type=<class 'str'>, choices=['RSNA', 'RHPE'], help='Dataset to perform training', metavar=None)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parser.add_argument('--data-train', default='data/train/', type=str,\n",
        "                help='path to train data folder')\n",
        "parser.add_argument('--ann-path-train', default='train.csv', type=str,\n",
        "                help='path to BAA annotations file')\n",
        "parser.add_argument('--rois-path-train', default='train.json',\n",
        "                type=str, help='path to ROIs annotations in coco format')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcAUbTLKdebn",
        "outputId": "0a7879ef-f3bd-4e8f-f1eb-b34b208cbdae"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['--rois-path-train'], dest='rois_path_train', nargs=None, const=None, default='train.json', type=<class 'str'>, choices=None, help='path to ROIs annotations in coco format', metavar=None)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parser.add_argument('--data-val', default='data/val/', type=str,\n",
        "                help='path to val data folder')\n",
        "parser.add_argument('--ann-path-val', default='val.csv', type=str,\n",
        "                help='path to BAA annotations file')\n",
        "parser.add_argument('--rois-path-val', default='val.json',\n",
        "                type=str, help='path to ROIs annotations in coco format')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vafkBPPdeYZ",
        "outputId": "d57ee00e-86e5-4ad0-8e88-89b9f19aa329"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['--rois-path-val'], dest='rois_path_val', nargs=None, const=None, default='val.json', type=<class 'str'>, choices=None, help='path to ROIs annotations in coco format', metavar=None)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parser.add_argument('--save-folder', default='TRAIN/new_test/',\n",
        "                help='location to save checkpoint models')\n",
        "parser.add_argument('--snapshot', default='boneage_bonet_weights.pth',\n",
        "                help='path to weight snapshot file')\n",
        "parser.add_argument('--optim-snapshot', type=str,\n",
        "                default='boneage_bonet_optim.pth',\n",
        "                help='path to optimizer state snapshot')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iO4C5MJIdeV5",
        "outputId": "a28c90ae-d4a1-41ec-c0b8-431b2e91b602"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['--optim-snapshot'], dest='optim_snapshot', nargs=None, const=None, default='boneage_bonet_optim.pth', type=<class 'str'>, choices=None, help='path to optimizer state snapshot', metavar=None)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parser.add_argument('--eval-first', default=False, action='store_true',\n",
        "                help='evaluate model weights before training')\n",
        "parser.add_argument('-j', '--workers', default=4, type=int, metavar='N',\n",
        "                help='number of data loading workers (default: 4)')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0feMpLkKdeS6",
        "outputId": "a02bf75f-5863-43e5-a64d-6b50719d47a1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['-j', '--workers'], dest='workers', nargs=None, const=None, default=4, type=<class 'int'>, choices=None, help='number of data loading workers (default: 4)', metavar='N')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training procedure settings\n",
        "parser.add_argument('--batch-size', default=1, type=int,\n",
        "                help='Batch size for training')\n",
        "parser.add_argument('--epochs', type=int, default=20,\n",
        "                help='upper epoch limit')\n",
        "parser.add_argument('--lr', '--learning-rate', default=1e-5, type=float,\n",
        "                help='initial learning rate')\n",
        "parser.add_argument('--patience', default=2, type=int,\n",
        "                help='patience epochs for LR decreasing')\n",
        "parser.add_argument('--start-epoch', type=int, default=1,\n",
        "                help='epoch number to resume')\n",
        "parser.add_argument('--seed', type=int, default=1111,\n",
        "                    help='random seed')\n",
        "parser.add_argument('--log-interval', type=int, default=30, metavar='N',\n",
        "                    help='report interval')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZrM22jvdeQP",
        "outputId": "b550ca19-4d64-4e13-ffe7-e7d467c80205"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['--log-interval'], dest='log_interval', nargs=None, const=None, default=30, type=<class 'int'>, choices=None, help='report interval', metavar='N')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parser.add_argument('--gpu', type=str, default='2,3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RCl51X2c_tT",
        "outputId": "8dedabef-6655-4763-c4a7-d0127fccbd5f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['--gpu'], dest='gpu', nargs=None, const=None, default='2,3', type=<class 'str'>, choices=None, help=None, metavar=None)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "args = parser.parse_args()"
      ],
      "metadata": {
        "id": "Nw5L4_pofe9W",
        "outputId": "2739f100-f7f9-4b7a-d2e9-e0c3a8ae765c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: ipykernel_launcher.py [-h] [--heatmaps] [--cropped]\n",
            "                             [--dataset {RSNA,RHPE}] [--data-train DATA_TRAIN]\n",
            "                             [--ann-path-train ANN_PATH_TRAIN]\n",
            "                             [--rois-path-train ROIS_PATH_TRAIN]\n",
            "                             [--data-val DATA_VAL]\n",
            "                             [--ann-path-val ANN_PATH_VAL]\n",
            "                             [--rois-path-val ROIS_PATH_VAL]\n",
            "                             [--save-folder SAVE_FOLDER] [--snapshot SNAPSHOT]\n",
            "                             [--optim-snapshot OPTIM_SNAPSHOT] [--eval-first]\n",
            "                             [-j N] [--batch-size BATCH_SIZE]\n",
            "                             [--epochs EPOCHS] [--lr LR] [--patience PATIENCE]\n",
            "                             [--start-epoch START_EPOCH] [--seed SEED]\n",
            "                             [--log-interval N] [--gpu GPU]\n",
            "ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-e0fd8a52-c8ee-4fe4-a839-fe3c9ac239eb.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "args,unknown = parser.parse_known_args()"
      ],
      "metadata": {
        "id": "k56BZOL4c_ql"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args_dict = vars(args)\n",
        "print('Argument list to program')\n",
        "print('\\n'.join(['--{0} {1}'.format(arg, args_dict[arg])\n",
        "                 for arg in args_dict]))\n",
        "print('\\n\\n')\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n",
        "\n",
        "if not os.path.exists(args.save_folder):\n",
        "    os.makedirs(args.save_folder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IASnPVSLc_n7",
        "outputId": "c4d84c9a-b8ce-4a25-a8bb-dc78df94a402"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Argument list to program\n",
            "--heatmaps False\n",
            "--cropped False\n",
            "--dataset RSNA\n",
            "--data_train data/train/\n",
            "--ann_path_train train.csv\n",
            "--rois_path_train train.json\n",
            "--data_val data/val/\n",
            "--ann_path_val val.csv\n",
            "--rois_path_val val.json\n",
            "--save_folder TRAIN/new_test/\n",
            "--snapshot boneage_bonet_weights.pth\n",
            "--optim_snapshot boneage_bonet_optim.pth\n",
            "--eval_first False\n",
            "--workers 4\n",
            "--batch_size 1\n",
            "--epochs 20\n",
            "--lr 1e-05\n",
            "--patience 2\n",
            "--start_epoch 1\n",
            "--seed 1111\n",
            "--log_interval 30\n",
            "--gpu 2,3\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Horovod settings\n",
        "hvd.init()\n",
        "torch.cuda.set_device(hvd.local_rank())\n",
        "torch.cuda.manual_seed(hvd.size())\n",
        "\n",
        "args.distributed = hvd.size() > 1\n",
        "args.rank = hvd.rank()\n",
        "args.size = hvd.size()"
      ],
      "metadata": {
        "id": "r3ZBOtApd4AJ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import Bonet\n",
        "from Bonet.models.bonet import BoNet"
      ],
      "metadata": {
        "id": "ECqIneBa3TG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CREATE THE NETWORK ARCHITECTURE AND LOAD THE BEST MODEL\n",
        "if args.heatmaps:\n",
        "    from models.bonet_heatmap import BoNet\n",
        "else:\n",
        "    from Bonet.models.bonet import BoNet\n",
        "\n",
        "net = BoNet()\n",
        "\n",
        "if args.rank == 0:\n",
        "    print('---> Number of params: {}'.format(\n",
        "        sum([p.data.nelement() for p in net.parameters()])))\n",
        "\n",
        "if osp.exists(args.snapshot):\n",
        "    model_to_load=args.snapshot\n",
        "else:\n",
        "    model_to_load=args.save_folder+'/'+args.snapshot\n",
        "\n",
        "if osp.exists(model_to_load) and args.rank == 0:\n",
        "    print('Loading state dict from: {0}'.format(model_to_load))\n",
        "    snapshot_dict = torch.load(model_to_load, map_location=lambda storage, loc: storage)\n",
        "    weights= net.state_dict()\n",
        "    new_snapshot_dict=snapshot_dict.copy()\n",
        "    for key in snapshot_dict:\n",
        "        if key not in weights.keys():\n",
        "            new_key='inception_v3.'+key\n",
        "            new_snapshot_dict[new_key]=snapshot_dict[key]\n",
        "            new_snapshot_dict.pop(key)\n",
        "\n",
        "    net.load_state_dict(new_snapshot_dict)\n",
        "\n",
        "net = net.to(device)"
      ],
      "metadata": {
        "id": "Ao0lvPyr3FS7",
        "outputId": "7ae42365-0794-4a89-a0ef-318f7a582611",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---> Number of params: 123172057\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Criterion\n",
        "criterion = nn.L1Loss()"
      ],
      "metadata": {
        "id": "vJc3Q4tQ3F_0"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizer\n",
        "optimizer = optim.Adam(net.parameters(), lr=args.lr * args.size)\n",
        "annealing = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, factor=0.8, patience=args.patience, cooldown=5,\n",
        "    min_lr=0.00001, eps=0.00001, verbose=True)\n",
        "\n",
        "if osp.exists(args.optim_snapshot):\n",
        "    optim_to_load=args.optim_snapshot\n",
        "else:\n",
        "    optim_to_load=args.save_folder+'/'+args.optim_snapshot\n",
        "\n",
        "if osp.exists(optim_to_load):\n",
        "    print('loading optim snapshot from {}'.format(optim_to_load))\n",
        "    optimizer.load_state_dict(torch.load(optim_to_load, map_location=lambda storage,\n",
        "                                             loc: storage))"
      ],
      "metadata": {
        "id": "qOmFS_ia3eZr"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Horovod\n",
        "hvd.broadcast_parameters(net.state_dict(), root_rank=0)\n",
        "\n",
        "optimizer = hvd.DistributedOptimizer(\n",
        "    optimizer, named_parameters=net.named_parameters())\n",
        "hvd.broadcast_optimizer_state(optimizer, root_rank=0)\n",
        "group = optimizer.param_groups[0]\n",
        "group['betas'] = (float(group['betas'][0]), float(group['betas'][1]))"
      ],
      "metadata": {
        "id": "ANYENDqq3htm"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataloaders\n",
        "train_transform = transforms.Compose([transforms.Resize((500, 500)),\n",
        "                               transforms.RandomAffine(\n",
        "                                   20, translate=(0.2, 0.2),\n",
        "                                   scale=(1, 1.2)),\n",
        "                               transforms.RandomHorizontalFlip(),\n",
        "                               transforms.ToTensor()])\n",
        "\n",
        "val_transform = transforms.Compose([transforms.Resize((500, 500)),\n",
        "                               transforms.ToTensor()])\n",
        "\n",
        "if args.heatmaps:\n",
        "    from data.data_loader import Boneage_HeatmapDataset as Dataset\n",
        "else:\n",
        "    from Bonet.data.data_loader import BoneageDataset as Dataset\n",
        "\n",
        "train_dataset = Dataset(args.data_train, args.ann_path_train,args.rois_path_train,\n",
        "                                   img_transform=train_transform,crop=args.cropped,dataset=args.dataset)\n",
        "val_dataset = Dataset(args.data_val, args.ann_path_val,args.rois_path_val,\n",
        "                                 img_transform=val_transform,crop=args.cropped,dataset=args.dataset)\n"
      ],
      "metadata": {
        "id": "3qOQf74w3mDO",
        "outputId": "8814581d-0571-4d36-a399-cfa51fb4e626",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-099c9cd7d0dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mBonet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBoneageDataset\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m train_dataset = Dataset(args.data_train, args.ann_path_train,args.rois_path_train,\n\u001b[0m\u001b[1;32m     18\u001b[0m                                    img_transform=train_transform,crop=args.cropped,dataset=args.dataset)\n\u001b[1;32m     19\u001b[0m val_dataset = Dataset(args.data_val, args.ann_path_val,args.rois_path_val,\n",
            "\u001b[0;32m/content/Bonet/data/data_loader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, img_dir, ann_file, json_file, img_transform, crop, dataset)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \"\"\"\n\u001b[1;32m    151\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mann_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#,dtype=object)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_transform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcrop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data samplers\n",
        "train_sampler = None\n",
        "val_sampler = None\n",
        "\n",
        "if args.distributed:\n",
        "    train_sampler = DistributedSampler(train_dataset,\n",
        "                                       num_replicas=args.size,\n",
        "                                       rank=args.rank)\n",
        "    val_sampler = DistributedSampler(val_dataset,\n",
        "                                     num_replicas=args.size,\n",
        "                                     rank=args.rank)\n",
        "\n",
        "train_loader = DataLoader(train_dataset,\n",
        "                             shuffle=(train_sampler is None),\n",
        "                             sampler=train_sampler,\n",
        "                             batch_size=args.batch_size,\n",
        "                             num_workers=args.workers)\n",
        "\n",
        "val_loader = DataLoader(val_dataset,\n",
        "                           shuffle=(val_sampler is None),\n",
        "                           sampler=val_sampler,\n",
        "                           batch_size=1,\n",
        "                           num_workers=args.workers)\n",
        "\n",
        "def main():\n",
        "    print('Train begins...')\n",
        "    best_val_loss = None\n",
        "    # Find best model in validation\n",
        "    if osp.exists(osp.join(args.save_folder, 'train.csv')):\n",
        "        with open(osp.join(args.save_folder, 'train.csv')) as csv_file:\n",
        "            csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "            val_list = []\n",
        "            for row in csv_reader:\n",
        "                val_list.append(float(row[2]))\n",
        "            best_val_loss = min(val_list)\n",
        "    if args.eval_first:\n",
        "        val_loss = evaluate()\n",
        "    try:\n",
        "        out_file = open(os.path.join(args.save_folder, 'train.csv'), 'a+')\n",
        "        \n",
        "        for epoch in range(args.start_epoch, args.epochs + 1):\n",
        "            if args.distributed:\n",
        "                train_sampler.set_epoch(epoch)\n",
        "                val_sampler.set_epoch(epoch)\n",
        "            if args.rank == 0:\n",
        "                epoch_start_time = time.time()\n",
        "            train_loss = train(epoch)\n",
        "            annealing.step(train_loss)\n",
        "            val_loss = evaluate()\n",
        "            if args.rank == 0:\n",
        "                print('-' * 89)\n",
        "                print('| end of epoch {:3d} | time: {:5.2f}s '\n",
        "                      '| epoch loss {:.6f} |'.format(\n",
        "                          epoch, time.time() - epoch_start_time, train_loss))\n",
        "                print('-' * 89)\n",
        "                out_file.write('{}, {}, {}\\n'.format(epoch, train_loss, val_loss))\n",
        "                out_file.flush()\n",
        "\n",
        "                if best_val_loss is None or val_loss > best_val_loss and args.rank == 0:\n",
        "                    best_val_loss = val_loss\n",
        "                    filename = osp.join(args.save_folder, 'boneage_bonet_weights.pth')\n",
        "                    torch.save(net.state_dict(), filename)\n",
        "        out_file.close()\n",
        "    except KeyboardInterrupt:\n",
        "        print('-' * 89)\n",
        "        print('Exiting from training early')\n",
        "\n",
        "def train(epoch):\n",
        "    net.train()\n",
        "    total_loss = AverageMeter()\n",
        "    epoch_loss_stats = AverageMeter()\n",
        "    time_stats = AverageMeter()\n",
        "    loss = 0\n",
        "    optimizer.zero_grad()\n",
        "    for (batch_idx, (imgs, bone_ages, genders, _)) in enumerate(train_loader):\n",
        "        imgs = imgs.to(device)\n",
        "        bone_ages = bone_ages.to(device)\n",
        "        genders = genders.to(device)\n",
        "\n",
        "        start_time = time.time()\n",
        "        outputs = net(imgs, genders)\n",
        "        loss = criterion(outputs.squeeze(), bone_ages)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss = metric_average(loss.item(), 'loss')\n",
        "\n",
        "        time_stats.update(time.time() - start_time, 1)\n",
        "        total_loss.update(loss, 1)\n",
        "        epoch_loss_stats.update(loss, 1)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if (batch_idx % args.log_interval == 0) and args.rank == 0:\n",
        "            elapsed_time = time_stats.avg\n",
        "            print(' [{:5d}] ({:5d}/{:5d}) | ms/batch {:.4f} |'\n",
        "                  ' loss {:.6f} | avg loss {:.6f} | lr {:.7f}'.format(\n",
        "                      epoch, batch_idx, len(train_loader),\n",
        "                      elapsed_time * 1000, total_loss.avg,\n",
        "                      epoch_loss_stats.avg,\n",
        "                      optimizer.param_groups[0]['lr']))\n",
        "            total_loss.reset()\n",
        "\n",
        "    epoch_total_loss = epoch_loss_stats.avg\n",
        "    args.resume_iter = 0\n",
        "\n",
        "    if args.rank == 0:\n",
        "        filename = 'boneage_bonet_snapshot.pth'\n",
        "        filename = osp.join(args.save_folder, filename)\n",
        "        torch.save(net.state_dict(), filename)\n",
        "\n",
        "        optim_filename = 'boneage_bonet_optim.pth'\n",
        "        optim_filename = osp.join(args.save_folder, optim_filename)\n",
        "        torch.save(optimizer.state_dict(), optim_filename)\n",
        "\n",
        "    return epoch_total_loss\n",
        "\n",
        "\n",
        "def evaluate():\n",
        "    net.eval()\n",
        "    epoch_total_loss = AverageMeter()\n",
        "    for (batch_idx, (imgs, bone_ages, genders, _)) in enumerate(val_loader):\n",
        "        imgs = imgs.to(device)\n",
        "        bone_ages = bone_ages.to(device)\n",
        "        genders = genders.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = net(imgs, genders)\n",
        "        loss = criterion(outputs.squeeze(), bone_ages)\n",
        "        loss = metric_average(loss.item(), 'loss')\n",
        "        epoch_total_loss.update(loss, 1)\n",
        "\n",
        "    epoch_total_loss = epoch_total_loss.avg\n",
        "\n",
        "    if args.rank == 0:\n",
        "        print('Val loss: {:.5f}'.format(epoch_total_loss))\n",
        "\n",
        "    return epoch_total_loss\n",
        "\n",
        "def metric_average(val, name):\n",
        "    tensor = torch.tensor(val)\n",
        "    avg_tensor = hvd.allreduce(tensor, name=name)\n",
        "    return avg_tensor.item()\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "7FQ-LX9k3sEF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}